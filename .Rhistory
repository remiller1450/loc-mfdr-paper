labs(x = expression(lambda), y = expression(widehat(mfdr))) + theme(legend.position="bottom") + ggtitle("")
## Lower left panel plot
long.coef <- melt(fit$fit$beta[-1,])
coef.path <- data.frame(long.coef, col = cols)
p2 <- ggplot() + geom_line(data = coef.path, aes(x = Var2, y = value, group = Var1, col = factor(col)), size = 1) + scale_x_reverse() +
scale_color_manual(name = "Feature Type", values = cids[c(1,3,2)], labels = c("Noise", "Correlated", "Causal")) +
geom_vline(xintercept=fit$lambda.min, linetype = "dotted") +
labs(x = expression(lambda), y = "Coefficient Estimate") + ggtitle("")
### Reset for upper panel plots using larger signal
set.seed(31236) ## Get the same X matrix using genData, but use a different beta vector
bb <- c(4,0,-4,0)
## Generate data
D1 <- genData(n, J=2, J0=2, K=2, K0=1, rho=0, rho.g=0.8, beta=bb)  #### 9 correlated vars for each true (type B)
D2 <- genData(n, p - 4, rho=0, beta=0, corr="auto")
X <- cbind(D1$X, .585*D2$X)
y <- D1$y
fit <- cv.ncvreg(X,y, penalty = "lasso", returnX = TRUE, seed = 123)
lseq <- fit$lambda
## Local mfdr for each lambda
locfdr.res <- matrix(NA, nrow = ncol(X), ncol = length(lseq))
sel.mat <- matrix(NA, nrow = ncol(X), ncol = length(lseq))
for(i in 1:length(lseq)){
temp <- locmfdr.plot1(fit$fit, lambda = lseq[i], number = ncol(X))
locfdr.res[,i] <- temp$locfdr
sel.mat[,i] <- (temp$selected == '*')
}
### Format results for plotting
colnames(locfdr.res) <- lseq
wide <- data.frame(var = rownames(temp), col = cols, locfdr.res)
long <- melt(wide, id.vars = c("var","col"))
long$lambda<- -as.numeric(substr(long$variable,2,10))
wide.sel <- data.frame(var = rownames(temp), sel.mat)
long.sel <- melt(wide.sel, id.vars = c('var'))
long <- data.frame(long, sel = long.sel[,3])
nexts <- which(diff(long[order(long$var),]$sel) == 1) + 1
nexts.pos <- numeric(nrow(long))
nexts.pos[nexts] <- 1
long <- long[order(long$var),]
#### Univariate results
z <- numeric(p)
for(i in 1:p){
t <- summary(lm(y ~ X[,i]))$coefficients[2,3]
z[i] <- qnorm(pt(t,p-2))
}
d <- density(z)
dd <- approxfun(d$x, d$y)
fdr.u <- dnorm(z)/dd(z)
fdr.u[fdr.u > 1] <- 1
fdr.points <- data.frame(value = fdr.u, lambda = -(max(lseq)+.25), col = cols)
## Upper Right plot
p3 <- ggplot() +
geom_line(data = rbind(long[long$sel,],long[nexts,]),aes(x=-lambda, y=value, group=var, col = factor(col)), size=1, linetype = "solid") +
geom_line(data = rbind(long[!long$sel,], long[nexts,]), aes(x=-lambda, y=value, group=var, col = factor(col)), size=1, linetype = "dashed") +
geom_vline(xintercept=fit$lambda.min, linetype = "dotted") +
scale_x_reverse(limits=c((max(lseq)+.25),0)) +
geom_point(data = fdr.points, mapping = aes(x = -lambda, y = value, col = factor(col)), shape = 2, size = 1.5) +
scale_color_manual(name = "Feature Type", values = cids[c(1,3,2)], labels = c("Noise", "Correlated", "Causal")) +
labs(x = expression(lambda), y = expression(widehat(mfdr))) + theme(legend.position="bottom") + ggtitle("mfdr Path")
## Upper Left plot
long.coef <- melt(fit$fit$beta[-1,])
coef.path <- data.frame(long.coef, col = cols)
p4 <- ggplot() + geom_line(data = coef.path, aes(x = Var2, y = value, group = Var1, col = factor(col)), size = 1) + scale_x_reverse() +
scale_color_manual(name = "Feature Type", values = cids[c(1,3,2)], labels = c("Noise", "Correlated", "Causal")) +
geom_vline(xintercept=fit$lambda.min, linetype = "dotted") +
labs(x = expression(lambda), y = "Coefficient Estimate") + ggtitle("Coefficient Path")
## Setup common legend for all plots
g_legend<-function(a.gplot){
tmp <- ggplot_gtable(ggplot_build(a.gplot))
leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
legend <- tmp$grobs[[leg]]
return(legend)}
mylegend<-g_legend(p1)
grid.arrange(arrangeGrob(p4 + theme(legend.position="none"),
p3 + theme(legend.position="none"),
p2 + theme(legend.position="none"),
p1 + theme(legend.position="none"),
nrow=2),
mylegend, nrow=2,heights=c(10, 1))
## Assemble the 4 plots into a single figure and store
png("figures_tables/Fig1.png", h=7, w=8.5, units = 'in', res = 300)
grid.arrange(arrangeGrob(p4 + theme(legend.position="none"),
p3 + theme(legend.position="none"),
p2 + theme(legend.position="none"),
p1 + theme(legend.position="none"),
nrow=2),
mylegend, nrow=2,heights=c(10, 1))
dev.off()
### Set Seed
set.seed(13351)
### Setup parameters
n <- 200
p <- 100
bb <- c(3.5,0,-3.5,0)
### Generate data using funs sourced
D1 <- genData(n, J=2, J0=2, K=2, K0=1, rho=0, rho.g=0.83, beta=bb)  #### 9 correlated vars for each true (type B)
D2 <- genData(n, p - 4, rho=0, beta=0, corr="auto")
X <- cbind(D1$X, .6*D2$X)
y <- D1$y
## Fit cross-validated model
fit <- cv.ncvreg(X,y, penalty = "lasso", returnX = TRUE, seed = 123)
lseq <- fit$lambda
## Find loc mfdrs for all lambda values
locfdr.res <- matrix(NA, nrow = ncol(X), ncol = length(lseq))
sel.mat <- matrix(NA, nrow = ncol(X), ncol = length(lseq))
for(i in 1:length(lseq)){
temp <- locmfdr.plot1(fit$fit, lambda = lseq[i], number = ncol(X))
locfdr.res[,i] <- temp$locfdr
sel.mat[,i] <- (temp$selected == '*')
}
## Setup color pal for plotting
cols <- numeric(p)
cids <- pal(3)
cols[c(1,3)] <- cids[1]
cols[c(2,4)] <- cids[2]
cols[5:p] <- cids[3]
### Format results for plotting
colnames(locfdr.res) <- lseq
wide <- data.frame(var = rownames(temp), col = cols, locfdr.res)
long <- melt(wide, id.vars = c("var","col"))
long$lambda<- -as.numeric(substr(long$variable,2,10))
wide.sel <- data.frame(var = rownames(temp), sel.mat)
long.sel <- melt(wide.sel, id.vars = c('var'))
long <- data.frame(long, sel = long.sel[,3])
nexts <- which(diff(long[order(long$var),]$sel) == 1) + 1
nexts.pos <- numeric(nrow(long))
nexts.pos[nexts] <- 1
long <- long[order(long$var),]
#### Get Univariate Testing results
z <- numeric(p)
for(i in 1:p){
t <- summary(lm(y ~ X[,i]))$coefficients[2,3]
z[i] <- qnorm(pt(t,p-2))
}
d <- density(z)
dd <- approxfun(d$x, d$y)
fdr.u <- dnorm(z)/dd(z)
fdr.u[fdr.u > 1] <- 1
fdr.points <- data.frame(value = fdr.u, lambda = -(max(lseq)+.25), col = cols)
## Upper right panel plot
p1 <- ggplot() +
geom_line(data = rbind(long[long$sel,],long[nexts,]),aes(x=-lambda, y=value, group=var, col = factor(col)), size=1, linetype = "solid") +
geom_line(data = rbind(long[!long$sel,], long[nexts,]), aes(x=-lambda, y=value, group=var, col = factor(col)), size=1, linetype = "dashed") +
geom_vline(xintercept=fit$lambda.min, linetype = "dotted") +
scale_x_reverse(limits=c((max(lseq)+.25),0)) +
geom_point(data = fdr.points, mapping = aes(x = -lambda, y = value, col = factor(col)), shape = 2, size = 1.5) +
scale_color_manual(name = "Feature Type", values = cids[c(1,3,2)], labels = c("Noise", "Correlated", "Causal")) +
labs(x = expression(lambda), y = expression(widehat(mfdr))) + theme(legend.position="bottom") + ggtitle("")
## Lower left panel plot
long.coef <- melt(fit$fit$beta[-1,])
coef.path <- data.frame(long.coef, col = cols)
p2 <- ggplot() + geom_line(data = coef.path, aes(x = Var2, y = value, group = Var1, col = factor(col)), size = 1) + scale_x_reverse() +
scale_color_manual(name = "Feature Type", values = cids[c(1,3,2)], labels = c("Noise", "Correlated", "Causal")) +
geom_vline(xintercept=fit$lambda.min, linetype = "dotted") +
labs(x = expression(lambda), y = "Coefficient Estimate") + ggtitle("")
### Reset for upper panel plots using larger signal
set.seed(31236) ## Get the same X matrix using genData, but use a different beta vector
bb <- c(4,0,-4,0)
## Generate data
D1 <- genData(n, J=2, J0=2, K=2, K0=1, rho=0, rho.g=0.8, beta=bb)  #### 9 correlated vars for each true (type B)
D2 <- genData(n, p - 4, rho=0, beta=0, corr="auto")
X <- cbind(D1$X, .585*D2$X)
y <- D1$y
fit <- cv.ncvreg(X,y, penalty = "lasso", returnX = TRUE, seed = 12)
lseq <- fit$lambda
## Local mfdr for each lambda
locfdr.res <- matrix(NA, nrow = ncol(X), ncol = length(lseq))
sel.mat <- matrix(NA, nrow = ncol(X), ncol = length(lseq))
for(i in 1:length(lseq)){
temp <- locmfdr.plot1(fit$fit, lambda = lseq[i], number = ncol(X))
locfdr.res[,i] <- temp$locfdr
sel.mat[,i] <- (temp$selected == '*')
}
### Format results for plotting
colnames(locfdr.res) <- lseq
wide <- data.frame(var = rownames(temp), col = cols, locfdr.res)
long <- melt(wide, id.vars = c("var","col"))
long$lambda<- -as.numeric(substr(long$variable,2,10))
wide.sel <- data.frame(var = rownames(temp), sel.mat)
long.sel <- melt(wide.sel, id.vars = c('var'))
long <- data.frame(long, sel = long.sel[,3])
nexts <- which(diff(long[order(long$var),]$sel) == 1) + 1
nexts.pos <- numeric(nrow(long))
nexts.pos[nexts] <- 1
long <- long[order(long$var),]
#### Univariate results
z <- numeric(p)
for(i in 1:p){
t <- summary(lm(y ~ X[,i]))$coefficients[2,3]
z[i] <- qnorm(pt(t,p-2))
}
d <- density(z)
dd <- approxfun(d$x, d$y)
fdr.u <- dnorm(z)/dd(z)
fdr.u[fdr.u > 1] <- 1
fdr.points <- data.frame(value = fdr.u, lambda = -(max(lseq)+.25), col = cols)
## Upper Right plot
p3 <- ggplot() +
geom_line(data = rbind(long[long$sel,],long[nexts,]),aes(x=-lambda, y=value, group=var, col = factor(col)), size=1, linetype = "solid") +
geom_line(data = rbind(long[!long$sel,], long[nexts,]), aes(x=-lambda, y=value, group=var, col = factor(col)), size=1, linetype = "dashed") +
geom_vline(xintercept=fit$lambda.min, linetype = "dotted") +
scale_x_reverse(limits=c((max(lseq)+.25),0)) +
geom_point(data = fdr.points, mapping = aes(x = -lambda, y = value, col = factor(col)), shape = 2, size = 1.5) +
scale_color_manual(name = "Feature Type", values = cids[c(1,3,2)], labels = c("Noise", "Correlated", "Causal")) +
labs(x = expression(lambda), y = expression(widehat(mfdr))) + theme(legend.position="bottom") + ggtitle("mfdr Path")
## Upper Left plot
long.coef <- melt(fit$fit$beta[-1,])
coef.path <- data.frame(long.coef, col = cols)
p4 <- ggplot() + geom_line(data = coef.path, aes(x = Var2, y = value, group = Var1, col = factor(col)), size = 1) + scale_x_reverse() +
scale_color_manual(name = "Feature Type", values = cids[c(1,3,2)], labels = c("Noise", "Correlated", "Causal")) +
geom_vline(xintercept=fit$lambda.min, linetype = "dotted") +
labs(x = expression(lambda), y = "Coefficient Estimate") + ggtitle("Coefficient Path")
## Setup common legend for all plots
g_legend<-function(a.gplot){
tmp <- ggplot_gtable(ggplot_build(a.gplot))
leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
legend <- tmp$grobs[[leg]]
return(legend)}
mylegend<-g_legend(p1)
## Assemble the 4 plots into a single figure and store
png("figures_tables/Fig1.png", h=7, w=8.5, units = 'in', res = 300)
grid.arrange(arrangeGrob(p4 + theme(legend.position="none"),
p3 + theme(legend.position="none"),
p2 + theme(legend.position="none"),
p1 + theme(legend.position="none"),
nrow=2),
mylegend, nrow=2,heights=c(10, 1))
dev.off()
?wilcox.test
?z.test
?prop.test
prop.test(15, 20, p = 0.5)
prop.test(15, 20, p = 0.5, correct = FALSe)
prop.test(15, 20, p = 0.5, correct = FALSE)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
tips = read.csv("https://remiller1450.github.io/data/Tips.csv")
tips = read.csv("https://remiller1450.github.io/data/Tips.csv")
tips$TipPercent = tips$Tip/tips$TotBill
tips = read.csv("https://remiller1450.github.io/data/OatBran.csv")
tips = read.csv("https://remiller1450.github.io/data/Oatbran.csv")
tips
set.seed(1234)
da <- data.frame(Participant = 1:19, Placebo_Placebo = round(rnorm(19, 17.22, 2.5),1),
Placebo_LowTHC = rbinom(19, 1, .56), Placebo_HighTHC = rbinom(19, 1, .65),
Alcohol_Placebo = rbinom(19, 1, .7), Alcohol_LowTHC = rbinom(19, 1, .71),
Alcohol_LowHigh= rbinom(19, 1, .76))
da[20,] <- apply(da, 2, sum)
da[20,1] <- "Total"
da[20,2] <- " "
colnames(da) <- c("Participant", "Baseline SDLP", "Placebo/Low THC", "Placebo/High THC", "Alcohol/Placebo", "Alcohol/Low THC", "Alcohol/High THC")
kable(da) %>% kable_styling()
library(kableExtra)
set.seed(1234)
da <- data.frame(Participant = 1:19, Placebo_Placebo = round(rnorm(19, 17.22, 2.5),1),
Placebo_LowTHC = rbinom(19, 1, .56), Placebo_HighTHC = rbinom(19, 1, .65),
Alcohol_Placebo = rbinom(19, 1, .7), Alcohol_LowTHC = rbinom(19, 1, .71),
Alcohol_LowHigh= rbinom(19, 1, .76))
da[20,] <- apply(da, 2, sum)
da[20,1] <- "Total"
da[20,2] <- " "
colnames(da) <- c("Participant", "Baseline SDLP", "Placebo/Low THC", "Placebo/High THC", "Alcohol/Placebo", "Alcohol/Low THC", "Alcohol/High THC")
kable(da) %>% kable_styling()
oats = read.csv("https://remiller1450.github.io/data/Oatbran.csv")
hist(oats$Difference)
hist(tips$TipPercent)
tips = read.csv("https://remiller1450.github.io/data/Tips.csv")
tips$TipPercent = tips$Tip/tips$TotBill ## Create tip percentage
hist(tips$TipPercent)
mean(tips$TipPercent)
dim(tips)
r256sec01f21 <- c("Adair, Benjamin Ransom","Calcaterra, Cormac Anthony","Druzina, Andrew Ignatious","Frenette, Anna Marie","Gardner, McKenzie Noel","Harley, Grace Marie","Holcomb, Brett James","Holderman, Matthew James","Hoying, Emmitt Emmitt","LaRosa, Abbie Rae","Leraris, Ben Pasquale","Marchese, Jacob Charles","McMyler, Emma Nicole","Peloquin, Mac McAllister","Raines, Brady J","Rathbone, Mick James","Ripperger, Jacob Michael","Rose, Abby Elyse","Simangan, Jaevee","Simms, Garfield","Solon, Jessica Christine","Taylor, Danyelle Baker","Touchette, Maddie Clare","Vandevelde, Ian Patrick","Vargas, Sonia","Vogel, Gracie Grace","Wentler, Lewis Curtis","Wilgress-Pipe, Duncan E","Wine, Ashton Camille Ann")
r146sec01f21 <- c('Casper, Mary Claire','Chen, Ramon','Collins, Annabella Rose', 'Dudley, Shelby Sanaa', 'Due, Eva Christine','Gholston-Green, Sofia Brooks', 'Goss, Jensyn Nicole','Haile, Sarah','Hearn, Davin Linscott','Jones, Taylor Mackenzie','Keeven, Clare Elisabeth','Kennard, Sydney Ana','Kuchinski, Kennedy Lynn','Losey, Dahlia Rae','Markert, Alex John','Meehan, Ryan Sullivan','Mullins, Michael Morris','Norris, Amy Michele','Prost, Hanna Taylormarie','Rudolph, Eleyna Gabrielle','Russell, Alex Danielle','Sitawi, Cailie Elizabeth','Slunski, Darby Samantha','Slywka, Emily Renae','Smith, Kelaiah Ari','Strege, Clay A','Tyler, Barbara Ann')
r146sec03f21 <- c("Anderson, Kay Brooke","Bauer, Emma", "Burek, Sam Michael","Cedillos, Ana M", "Dipnarine, Sara Ann","Fanning, Nathan Daniel","Fitzgerald, Jack Kincade","Garnes, Elizabeth Joy","Gordon, Alaysia Rontrice","Jordan, Zaira", "Keiser, Clara Celeste","Klei, Elise Nicole","Lampkin, Brianna Joann","Mackey, Brianna Logan","Mohan, Sofia Lakshmi","Ryan, Carley Dianna","Sappington, Laila Sade","Schildknecht, Victoria Josephine","Smith, Mitchell B","Suttner, Ashley Paige","Walker, Taee K`shunna", "Mahnoor, Zahra")
get_groups <- function(roster, size = 4){
n = ceiling(length(roster)/size)
v = rep(1:n, length.out = length(roster))
out = data.frame(Name = roster, Group = sample(v, replace = FALSE))
return(out)
}
g <- get_groups(r256sec01f21, size = 4)  ## 11:30am
## View groups
View(g)
## Randomly call on group
sample(1:max(g$Group), size = 1)
## Randomly call on group
sample(1:max(g$Group), size = 1)
## Randomly call on group
sample(1:max(g$Group), size = 1)
## H0: p = 0.70
## Ha: p =/= 0.70
## We observed an outcome of p-hat = 31/39
## Null distribution is N(0.7, sqrt(0.7*(1-0.7)/39))
pnorm(q = 31/39, mean = 0.7, sd = sqrt(0.7*(1-0.7)/39), lower.tail = FALSE)
## Double for a two-sided p-value
pnorm(q = 31/39, mean = 0.7, sd = sqrt(0.7*(1-0.7)/39), lower.tail = FALSE)
2*pnorm(q = 31/39, mean = 0.7, sd = sqrt(0.7*(1-0.7)/39), lower.tail = FALSE)
d1 = read.csv("https://remiller1450.github.io/data/Ohio_Counties_Cancer.csv")
d2 = read.csv("https://remiller1450.github.io/data/Midwest_Counties.csv")
d1
d1 = d1[,1:27]
d1
d1 = d1[,1:26]
d1
d2
d2 = subset(d2$state == "OH")
d2$County = d2$county
d2 = subset(d2, d2$state == "OH")
d1 = read.csv("https://remiller1450.github.io/data/Ohio_Counties_Cancer.csv")
d1 = d1[,1:26]
d2 = read.csv("https://remiller1450.github.io/data/Midwest_Counties.csv")
d2 = subset(d2, d2$state == "OH")
d2$County = d2$county
left_join(d1, d2, by = "County")
mm = left_join(d1, d2, by = "County")
select_var(mm, var = -c("PID"))
mm[, -"County"]
write.csv(mm, "C:\Users\millerr33\Documents\GitHub\remiller1450.github.io\data\oh_cancer.csv")
write.csv(mm, file = "C:\Users\millerr33\Documents\GitHub\remiller1450.github.io\data\oh_cancer.csv", row.names = FALSE)
write.csv(mm,
file = "C:\\Users\\millerr33\\Documents\\GitHub\\remiller1450.github.io\\data\\oh_cancer.csv",
row.names = FALSE)
d = read.csv("https://gist.githubusercontent.com/aishwarya8615/d2107f828d3f904839cbcb7eaa85bd04/raw/cec0340503d82d270821e03254993b6dede60afb/healthcare-dataset-stroke-data.csv")
d
d = subset(d, bmi != "N/A")
summary(d)
str(d)
table(d$hypertension)
table(d$smoking_status  )
table(d$stroke)
write.csv(d,
file = "C:\\Users\\millerr33\\Documents\\GitHub\\remiller1450.github.io\\data\\healthcare.csv",
row.names = FALSE)
sqrt(.3*.7/39)
(31/39)
(0.795 - 0.7)/0.07338
sqrt(0.33*0.66/120)
66/120
(0.55-0.33)/0.043
mean(.2,.7,.3,.9,.5,.3,.7,.6)
mean(c(.2,.7,.3,.9,.5,.3,.7,.6))
.077/(0.022/sqrt(12))
r256sec01f21 <- c("Adair, Benjamin Ransom","Calcaterra, Cormac Anthony","Druzina, Andrew Ignatious","Frenette, Anna Marie","Gardner, McKenzie Noel","Harley, Grace Marie","Holcomb, Brett James","Holderman, Matthew James","Hoying, Emmitt Emmitt","LaRosa, Abbie Rae","Leraris, Ben Pasquale","Marchese, Jacob Charles","McMyler, Emma Nicole","Peloquin, Mac McAllister","Raines, Brady J","Rathbone, Mick James","Ripperger, Jacob Michael","Rose, Abby Elyse","Simangan, Jaevee","Simms, Garfield","Solon, Jessica Christine","Taylor, Danyelle Baker","Touchette, Maddie Clare","Vandevelde, Ian Patrick","Vargas, Sonia","Vogel, Gracie Grace","Wentler, Lewis Curtis","Wilgress-Pipe, Duncan E","Wine, Ashton Camille Ann")
r146sec01f21 <- c('Casper, Mary Claire','Chen, Ramon','Collins, Annabella Rose', 'Dudley, Shelby Sanaa', 'Due, Eva Christine','Gholston-Green, Sofia Brooks', 'Goss, Jensyn Nicole','Haile, Sarah','Hearn, Davin Linscott','Jones, Taylor Mackenzie','Keeven, Clare Elisabeth','Kennard, Sydney Ana','Kuchinski, Kennedy Lynn','Losey, Dahlia Rae','Markert, Alex John','Meehan, Ryan Sullivan','Mullins, Michael Morris','Norris, Amy Michele','Prost, Hanna Taylormarie','Rudolph, Eleyna Gabrielle','Russell, Alex Danielle','Sitawi, Cailie Elizabeth','Slunski, Darby Samantha','Slywka, Emily Renae','Smith, Kelaiah Ari','Strege, Clay A','Tyler, Barbara Ann')
r146sec03f21 <- c("Anderson, Kay Brooke","Bauer, Emma", "Burek, Sam Michael","Cedillos, Ana M", "Dipnarine, Sara Ann","Fanning, Nathan Daniel","Fitzgerald, Jack Kincade","Garnes, Elizabeth Joy","Gordon, Alaysia Rontrice","Jordan, Zaira", "Keiser, Clara Celeste","Klei, Elise Nicole","Lampkin, Brianna Joann","Mackey, Brianna Logan","Mohan, Sofia Lakshmi","Ryan, Carley Dianna","Sappington, Laila Sade","Schildknecht, Victoria Josephine","Smith, Mitchell B","Suttner, Ashley Paige","Walker, Taee K`shunna", "Mahnoor, Zahra")
get_groups <- function(roster, size = 4){
n = ceiling(length(roster)/size)
v = rep(1:n, length.out = length(roster))
out = data.frame(Name = roster, Group = sample(v, replace = FALSE))
return(out)
}
## Get groups
g <- get_groups(r146sec03f21, size = 4)  ## 8am
## View groups
View(g)
g <- get_groups(r146sec01f21, size = 5)  ## 9am
## View groups
View(g)
knitr::opts_chunk$set(echo = FALSE)
library(reshape2)
library(ggplot2)
library(knitr)
library(kableExtra)
library(dplyr)
binom.test(x = 14, n = 16, p = 0.5, alternative = "greater")
2*pnorm(3, lower.tail = FALSE)
pnorm(3, lower.tail = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
pk = read.csv("https://remiller1450.github.io/data/PoliceKillings.csv")
ws = read.csv("https://remiller1450.github.io/data/Wetsuits2.csv")
ws
ws = read.csv("https://remiller1450.github.io/data/Wetsuits2.csv")
ws
pt(.025, df = 14)
qt(.025, df = 14)
214 - 2.145*21/sqrt(15)
214 + 2.145*21/sqrt(15)
pk = read.csv("https://remiller1450.github.io/data/ab_data.csv")
park = read.csv("https://remiller1450.github.io/data/Parking.csv")
beers = read.csv("https://remiller1450.github.io/data/beers.csv")
beers = read.csv("https://remiller1450.github.io/data/beers.csv")
beers = read.csv("https://remiller1450.github.io/data/beers.csv")
beers
beers = read.csv("https://remiller1450.github.io/data/beers.csv")
t.test(x = beers$Difference)
hist(beers$Difference)
t.test(beers$Difference)
data.frame(Race = c(780 , 117 , 114 , 384 , 58))
kable(t(df)) %>% kable_styling()
df = data.frame(Race = c(780 , 117 , 114 , 384 , 58))
row.names(df) =  c(White , Black , Hispanic , Asian , Other)
df = data.frame(Race = c(780 , 117 , 114 , 384 , 58))
row.names(df) =  c("White" , "Black" , "Hispanic" , "Asian" , "Other")
kable(t(df)) %>% kable_styling()
df = data.frame(Count = c(780 , 117 , 114 , 384 , 58, 1453))
row.names(df) =  c("White" , "Black" , "Hispanic" , "Asian" , "Other", "Total")
kable(t(df)) %>% kable_styling()
## Library
library(readxl)
library(R.matlab)
library(dplyr)
library(stringr)
## Lead break
disp = read_excel("C:\\Users\\millerr33\\Documents\\CU_data\\disposition\\disposition_brake_slowing_03242020.xls")
## Only keep reduced
disp_red = subset(disp, Reduced == "X")
## Extract file names
file_list = str_replace_all(disp_red$DaqName, ".daq", "")
## Current file
ht = "C:\\Users\\millerr33\\Documents\\CU_data\\RawData\\Colorado Data\\Brake\\Slowing\\"
i = 2
tdata = readMat.default(paste0(ht, file_list[i], ".mat"))
var_ids = cbind(1:length(dimnames(tdata$elemDataI)[[1]]), dimnames(tdata$elemDataI)[[1]])
audio_id = as.numeric(var_ids[var_ids[,2] == "SCC.Audio.Trigger",1])
aud_frames = which(tdata[["elemDataI"]][audio_id][[1]] <= 414 & tdata[["elemDataI"]][audio_id][[1]] >= 400)
aud_frames
diff(aud_frames)
which(diff(aud_frames) != 1)
ldev_id = as.numeric(var_ids[var_ids[,2] == "SCC.Lane.Deviation",1])
plot(tdata[["elemDataI"]][ldev_id][[1]][,2])
abline(v = min(aud_frames))
abline(v = max(aud_frames))
min(tdata[["elemDataI"]][ldev_id][[1]][aud_frames,2])
which.min(tdata[["elemDataI"]][ldev_id][[1]][aud_frames,2])
which.max(tdata[["elemDataI"]][ldev_id][[1]][aud_frames,2])
length(aud_frames)
480/60
tdata[["elemDataI"]][audio_id][[1]]
table(tdata[["elemDataI"]][audio_id][[1]])
if (which.min(tdata[["elemDataI"]][ldev_id][[1]][aud_frames,2]) <
which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames),2]))
mm = which.min(tdata[["elemDataI"]][ldev_id][[1]][aud_frames, aud_frames - 1:60),2]) <
which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames + 1:60),2])
mm = which.min(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames - 1:60),2]) <
which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames + 1:60),2])
mm
which.min(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames - 1:60),2])
which.min(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames - 1:60),2]) <
which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames + 1:60),2])
which.min(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames - 1:60),2]) -
which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames + 1:60),2])
mf = which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames + 1:60),2])
cf = which.min(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames - 1:60),2])
mf = which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, aud_frames + 1:180),2])
cf = which.min(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, min(aud_frames) - 1:60),2])
cf
min(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, min(aud_frames) - 1:60),2])
c(aud_frames, min(aud_frames) - 1:60)
cf = which.min(tdata[["elemDataI"]][ldev_id][[1]][c( min(aud_frames) - 1:60, aud_frames),2])
c(min(aud_frames) - 1:60, aud_frames)
diff(c(min(aud_frames) - 1:60, aud_frames))
diff(c(min(aud_frames) - 60:1, aud_frames))
mf = which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, max(aud_frames) + 1:180),2])
mp = max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, max(aud_frames) + 1:180),2])
mp
cp
cp = min(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, aud_frames),2])
mp = max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, max(aud_frames) + 1:180),2])
cf = which.min(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, aud_frames),2])
mf = which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, max(aud_frames) + 1:180),2])
i = 3
tdata = readMat.default(paste0(ht, file_list[i], ".mat"))
## Find Events
var_ids = cbind(1:length(dimnames(tdata$elemDataI)[[1]]), dimnames(tdata$elemDataI)[[1]])
audio_id = as.numeric(var_ids[var_ids[,2] == "SCC.Audio.Trigger",1])
aud_frames = which(tdata[["elemDataI"]][audio_id][[1]] <= 414 & tdata[["elemDataI"]][audio_id][[1]] >= 400)
ldev_id = as.numeric(var_ids[var_ids[,2] == "SCC.Lane.Deviation",1])
plot(tdata[["elemDataI"]][ldev_id][[1]][,2])
abline(v = min(aud_frames))
abline(v = max(aud_frames))
which.min(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, aud_frames),2])
which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, max(aud_frames) + 1:180),2])
max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, max(aud_frames) + 1:180),2])
min(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, aud_frames),2])
sd(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, max(aud_frames) + 1:180),2])
sd(tdata[["elemDataI"]][ldev_id][[1]][c(cf, mf),2])
tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, max(aud_frames) + 1:180),2]
tdata[["elemDataI"]][ldev_id][[1]][c(cf, mf),2]
sd(tdata[["elemDataI"]][ldev_id][[1]][cf:mf,2])
tdata[["elemDataI"]][ldev_id][[1]][cf:mf,2]
cf = which.min(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, aud_frames),2])
mf = which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, max(aud_frames) + 1:180),2])
mf
tdata[["elemDataI"]][ldev_id][[1]][cf:mf,2]
min(aud_frames) - 60:1
table(c(min(aud_frames) - 60:1, max(aud_frames) + 1:180))
tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, max(aud_frames) + 1:180),2]
sd(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, max(aud_frames) + 1:180),2])
sd(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, max(aud_frames) + 1:180),2] - mean(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, max(aud_frames) + 1:180),2]))
sd(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, max(aud_frames) + 1:180),2])
cf
cf = c(min(aud_frames) - 60:1, aud_frames)[which.min(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, aud_frames),2])]
cf
mf = c(aud_frames, max(aud_frames) + 1:180)[which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, max(aud_frames) + 1:180),2])]
## Find frame of min and max pos dev
cf = c(min(aud_frames) - 60:1, aud_frames)[which.min(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, aud_frames),2])]
mf = c(aud_frames, max(aud_frames) + 1:180)[which.max(tdata[["elemDataI"]][ldev_id][[1]][c(aud_frames, max(aud_frames) + 1:180),2])]
sd(tdata[["elemDataI"]][ldev_id][[1]][cf:mf,2])
sd(tdata[["elemDataI"]][ldev_id][[1]][c(min(aud_frames) - 60:1, max(aud_frames) + 1:180),2])
var_ids
## Setup storage objects
pattern = rep("no_drift", length(file_list))
pattern
radon = c(.2,.7,.3,.9,.5,.3,.7,.6)
radon = c(.2,.7,.3,.9,.5,.3,.7,.6)
wilcox.test(x = radon)
wilcox.test(x = radon, mu = 0.4)
```{r, echo = TRUE, warning = FALSE}
radon = c(.2,.7,.3,.9,.5,.3,.7,.6)
wilcox.test(x = radon, mu = 0.4)
?wilcox.test
radon = c(.2,.7,.3,.9,.5,.3,.7,.6)
wilcox.test(x = radon, mu = 0.4, alternative = "greater")
